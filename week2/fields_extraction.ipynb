{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7133a18a",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07823428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8708280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\divya\\Documents\\GitHub\\Build_Projects_Open_Avenue\\week2\\open_ave_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ca22b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ReportText</th>\n",
       "      <th>findings</th>\n",
       "      <th>clinicaldata</th>\n",
       "      <th>ExamName</th>\n",
       "      <th>impression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 ...</td>\n",
       "      <td>FINDINGS: Lungs/Pleura: No focal opacities evi...</td>\n",
       "      <td>CLINICAL HISTORY: Cough. \\n\\n</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 ...</td>\n",
       "      <td>IMPRESSION: Normal 2-view chest radiography.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 05/23/2020 ...</td>\n",
       "      <td>FINDINGS: Lungs/Pleura: No focal opacities evi...</td>\n",
       "      <td>CLINICAL HISTORY: CHEST PAIN. \\n\\n</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 05/23/2020 ...</td>\n",
       "      <td>IMPRESSION: No acute cardiopulmonary abnormali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 12/13/2019 ...</td>\n",
       "      <td>FINDINGS: Lungs/Pleura: No focal opacities evi...</td>\n",
       "      <td>CLINICAL HISTORY: CHEST PAIN. \\n\\n</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 12/13/2019 ...</td>\n",
       "      <td>IMPRESSION: No acute cardiopulmonary process.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Exam: - CHEST-PORTABLE History: Chest pain Com...</td>\n",
       "      <td>Findings: Heart size appears normal. Lungs cle...</td>\n",
       "      <td>History: Chest pain \\n\\n</td>\n",
       "      <td>Exam: - CHEST-PORTABLE\\n\\nComparison: None</td>\n",
       "      <td>Impression: Lungs clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/17/2021 ...</td>\n",
       "      <td>FINDINGS: Lungs/Pleura: No focal opacities evi...</td>\n",
       "      <td>CLINICAL HISTORY: CHEST PAIN, SHORTNESS OF BRE...</td>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/17/2021 ...</td>\n",
       "      <td>IMPRESSION: Normal single view chest.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         ReportText  \\\n",
       "0           0  EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 ...   \n",
       "1           1  EXAM: CHEST RADIOGRAPHY EXAM DATE: 05/23/2020 ...   \n",
       "2           2  EXAM: CHEST RADIOGRAPHY EXAM DATE: 12/13/2019 ...   \n",
       "3           3  Exam: - CHEST-PORTABLE History: Chest pain Com...   \n",
       "4           4  EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/17/2021 ...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  FINDINGS: Lungs/Pleura: No focal opacities evi...   \n",
       "1  FINDINGS: Lungs/Pleura: No focal opacities evi...   \n",
       "2  FINDINGS: Lungs/Pleura: No focal opacities evi...   \n",
       "3  Findings: Heart size appears normal. Lungs cle...   \n",
       "4  FINDINGS: Lungs/Pleura: No focal opacities evi...   \n",
       "\n",
       "                                        clinicaldata  \\\n",
       "0                      CLINICAL HISTORY: Cough. \\n\\n   \n",
       "1                 CLINICAL HISTORY: CHEST PAIN. \\n\\n   \n",
       "2                 CLINICAL HISTORY: CHEST PAIN. \\n\\n   \n",
       "3                           History: Chest pain \\n\\n   \n",
       "4  CLINICAL HISTORY: CHEST PAIN, SHORTNESS OF BRE...   \n",
       "\n",
       "                                            ExamName  \\\n",
       "0  EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 ...   \n",
       "1  EXAM: CHEST RADIOGRAPHY EXAM DATE: 05/23/2020 ...   \n",
       "2  EXAM: CHEST RADIOGRAPHY EXAM DATE: 12/13/2019 ...   \n",
       "3         Exam: - CHEST-PORTABLE\\n\\nComparison: None   \n",
       "4  EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/17/2021 ...   \n",
       "\n",
       "                                          impression  \n",
       "0      IMPRESSION: Normal 2-view chest radiography.   \n",
       "1  IMPRESSION: No acute cardiopulmonary abnormali...  \n",
       "2     IMPRESSION: No acute cardiopulmonary process.   \n",
       "3                            Impression: Lungs clear  \n",
       "4             IMPRESSION: Normal single view chest.   "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b3c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels='Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2b6195e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReportText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 05/23/2020 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 12/13/2019 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exam: - CHEST-PORTABLE History: Chest pain Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/17/2021 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ReportText\n",
       "0  EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 ...\n",
       "1  EXAM: CHEST RADIOGRAPHY EXAM DATE: 05/23/2020 ...\n",
       "2  EXAM: CHEST RADIOGRAPHY EXAM DATE: 12/13/2019 ...\n",
       "3  Exam: - CHEST-PORTABLE History: Chest pain Com...\n",
       "4  EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/17/2021 ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=data[[\"ReportText\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce342a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=df.to_numpy().tolist()\n",
    "l=l[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f168148",
   "metadata": {},
   "source": [
    "#### Setting up LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f58d2f",
   "metadata": {},
   "source": [
    "##### loading environ variable / API Keys to use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c7f1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "659676db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a071d",
   "metadata": {},
   "source": [
    "#### Prompt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54e6c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa7d7a",
   "metadata": {},
   "source": [
    "#### Pydantic Class setup  and output parser\n",
    "\n",
    "- For Data validation\n",
    "- To ensure the output is in the required JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a757c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52fe4869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ReportText', 'findings', 'clinicaldata', 'ExamName', 'impression'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dbd0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldsExtraction(BaseModel):\n",
    "    findings:str=Field(description=\"Radiologist's technical observations\")\n",
    "    clinicaldata:str=Field(description=\"Reason for examination (e.g., symptoms like chest pain, shortness of breath)\")\n",
    "    ExamName:str=Field(description=\"Exam type and date\")\n",
    "    impression:str=Field(description=\"Final diagnosis or summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ba7adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser=PydanticOutputParser(pydantic_object=FieldsExtraction)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "output_parser.get_format_instructions()\n",
    "result=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d7455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful medical data extraction assistant. \n",
    "\n",
    "            From the given \"Report Text\", extract the following fields and return ONLY a JSON object, and nothing else. \n",
    "            Use the format described in {format_instructions}. Do not add explanations, Python code, or markdown formatting. \n",
    "            Return exactly this structure:\n",
    "         \n",
    "            Only return a JSON object with the following fields:\n",
    "\n",
    "            ### Fields to be extracted:\n",
    "            - findings\n",
    "            - clinicaldata\n",
    "            - ExamName\n",
    "            - impression\n",
    "\n",
    "            ### Example:\n",
    "\n",
    "            Input:\n",
    "            EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 08:30 PM. CLINICAL HISTORY: Cough. COMPARISON: None. TECHNIQUE: 2 views. FINDINGS: Lungs/Pleura: No focal opacities evident. No pleural effusion. No pneumothorax. Normal volumes. Mediastinum: Heart and mediastinal contours are unremarkable. Other: None. IMPRESSION: Normal 2-view chest radiography Dictated by: [[PERSONALNAME]] on 06/01/2019 08:42 PM. Electronically signed by: [[PERSONALNAME]] on 06/01/2019 08:43 PM.\n",
    "\n",
    "            Extracted:\n",
    "            findings = FINDINGS: Lungs/Pleura: No focal opacities evident. No pleural effusion. No pneumothorax. Normal volumes. Mediastinum: Heart and mediastinal contours are unremarkable. Other: None.  \n",
    "            clinicaldata = CLINICAL HISTORY: Cough.  \n",
    "            ExamName = EXAM: CHEST RADIOGRAPHY EXAM DATE: 06/01/2019 08:30 PM. TECHNIQUE: 2 views. COMPARISON: None.  \n",
    "            impression = IMPRESSION: Normal 2-view chest radiography    Dictated by: [[PERSONALNAME]] on 06/01/2019 08:42 PM. Electronically signed by: [[PERSONALNAME]] on 06/01/2019 08:43 PM.\n",
    "         \n",
    "            ### NOTE: Return all fields as flat strings — do not nest them or break into subfields like 'examdate' or 'examname'. Return 'ExamName' as a single string, just as found in the report. \"\"\"),\n",
    "                \n",
    "                \n",
    "            (\"user\", \"{input}\") ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d07790",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9db571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Number of requests per minute (to stay below the limit)\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "DELAY_BETWEEN_BATCHES = 60  # seconds\n",
    "\n",
    "async def process_async(index, text):\n",
    "    try:\n",
    "        response = await chain.ainvoke({\n",
    "            \"input\": text,\n",
    "            \"format_instructions\": output_parser.get_format_instructions()\n",
    "        })\n",
    "        return index, response\n",
    "    except OutputParserException as e:\n",
    "        print(f\"[OutputParseError] Index {index}:\\n\", e.llm_output)\n",
    "        return index, None\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Index {index}: {e}\")\n",
    "        return index, None\n",
    "\n",
    "async def run_all(text_list, start_idx=0):\n",
    "    tasks = [process_async(start_idx + i, text) for i, text in enumerate(text_list)]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def run_batched(text_list):\n",
    "    total = len(text_list)\n",
    "    results = [None] * total\n",
    "    failed_indices = []\n",
    "\n",
    "    num_batches = math.ceil(total / BATCH_SIZE)\n",
    "\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start = batch_idx * BATCH_SIZE\n",
    "        end = min(start + BATCH_SIZE, total)\n",
    "        batch_texts = text_list[start:end]\n",
    "\n",
    "        print(f\"Running batch {batch_idx + 1}/{num_batches} ({len(batch_texts)} items)...\")\n",
    "\n",
    "        batch_results = await run_all(batch_texts, start_idx=start)\n",
    "\n",
    "        for i, res in enumerate(batch_results):\n",
    "            idx = start + i\n",
    "            results[idx] = res[1]\n",
    "            if res[1] is None:\n",
    "                failed_indices.append(idx)\n",
    "\n",
    "        if batch_idx < num_batches - 1:\n",
    "            print(f\"Waiting {DELAY_BETWEEN_BATCHES}s before next batch...\")\n",
    "            await asyncio.sleep(DELAY_BETWEEN_BATCHES)\n",
    "\n",
    "    # Retry logic (optional)\n",
    "    \n",
    "    if failed_indices:\n",
    "        print(f\"\\nRetrying {len(failed_indices)} failed items...\")\n",
    "        retry_texts = [text_list[i] for i in failed_indices]\n",
    "        retry_results = await run_all(retry_texts, start_idx=0)\n",
    "        for i, (idx, res) in enumerate(zip(failed_indices, retry_results)):\n",
    "            results[idx] = res[1]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2492ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch 1/1 (10 items)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OutputParseError] Index 7:\n",
      " null\n",
      "\n",
      "Retrying 1 failed items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OutputParseError] Index 0:\n",
      " null\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "final_results = await run_batched(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "435392e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to 'extracted_medical_reports.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "cleaned_results = [res.dict() if res else {} for res in final_results]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cleaned_results)\n",
    "\n",
    "\n",
    "df.to_csv(\"extracted_medical_reports_new.csv\", index=False)\n",
    "\n",
    "print(\"Saved to 'extracted_medical_reports.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a56150",
   "metadata": {},
   "source": [
    "### Evaluation of extracted fields with ground truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b3343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df=pd.read_csv(r'C:\\Users\\divya\\Documents\\GitHub\\Build_Projects_Open_Avenue\\week2\\Final_extracted_medical_reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d954ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df=pd.read_csv(r'C:\\Users\\divya\\Documents\\GitHub\\Build_Projects_Open_Avenue\\week2\\ground_truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312658f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"findings\", \"clinicaldata\", \"ExamName\", \"impression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c68b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:12<00:00, 14.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 134.31 seconds, 7.10 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:32<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      " 27%|██▋       | 4/15 [00:00<00:00, 29.81it/s]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      " 47%|████▋     | 7/15 [00:00<00:00, 28.92it/s]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      " 73%|███████▎  | 11/15 [00:00<00:00, 33.36it/s]Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "100%|██████████| 15/15 [00:00<00:00, 33.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 32.83 seconds, 29.06 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [03:36<00:00,  9.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 217.38 seconds, 4.39 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:55<00:00, 10.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 116.06 seconds, 8.22 sentences/sec\n",
      "          Field  BERTScore Precision  BERTScore Recall  BERTScore F1\n",
      "0      findings                99.52             99.33         99.42\n",
      "1  clinicaldata                98.55             98.27         98.40\n",
      "2      ExamName                95.46             94.86         95.13\n",
      "3    impression                99.09             98.40         98.72\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score as bert_score\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure all values are strings (replace NaN/float with empty string)\n",
    "def safe_text(x):\n",
    "    return str(x).strip().lower() if isinstance(x, str) else \"\"\n",
    "\n",
    "bert_results = []\n",
    "\n",
    "for field in fields:\n",
    "    references = [safe_text(x) for x in ground_truth_df[field]]\n",
    "    candidates = [safe_text(x) for x in extracted_df[field]]\n",
    "    \n",
    "    P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
    "\n",
    "    bert_results.append({\n",
    "        \"Field\": field,\n",
    "        \"BERTScore Precision\": round(P.mean().item() * 100, 2),\n",
    "        \"BERTScore Recall\": round(R.mean().item() * 100, 2),\n",
    "        \"BERTScore F1\": round(F1.mean().item() * 100, 2)\n",
    "    })\n",
    "\n",
    "bert_df = pd.DataFrame(bert_results)\n",
    "print(bert_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001921c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Field  ROUGE-1 F1  ROUGE-L F1\n",
      "0      findings       98.11       98.02\n",
      "1  clinicaldata       95.84       95.77\n",
      "2      ExamName       87.12       82.35\n",
      "3    impression       96.28       96.28\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to clean and standardize text inputs\n",
    "def safe_text(x):\n",
    "    return str(x).strip().lower() if isinstance(x, str) else \"\"\n",
    "\n",
    "# Re-initialize the scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "rouge_results = []\n",
    "\n",
    "# Compute ROUGE scores with safe text conversion\n",
    "for field in fields:\n",
    "    references = [safe_text(x) for x in ground_truth_df[field]]\n",
    "    predictions = [safe_text(x) for x in extracted_df[field]]\n",
    "    \n",
    "    scores = [rouge.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "    \n",
    "    avg_rouge1 = sum([s['rouge1'].fmeasure for s in scores]) / len(scores)\n",
    "    avg_rougeL = sum([s['rougeL'].fmeasure for s in scores]) / len(scores)\n",
    "    \n",
    "    rouge_results.append({\n",
    "        \"Field\": field,\n",
    "        \"ROUGE-1 F1\": round(avg_rouge1 * 100, 2),\n",
    "        \"ROUGE-L F1\": round(avg_rougeL * 100, 2)\n",
    "    })\n",
    "\n",
    "rouge_df = pd.DataFrame(rouge_results)\n",
    "print(rouge_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c2821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field</th>\n",
       "      <th>Exact Match Accuracy (%)</th>\n",
       "      <th>Average Fuzzy Match Score (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>findings</td>\n",
       "      <td>76.52</td>\n",
       "      <td>99.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinicaldata</td>\n",
       "      <td>83.23</td>\n",
       "      <td>98.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ExamName</td>\n",
       "      <td>0.63</td>\n",
       "      <td>99.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>impression</td>\n",
       "      <td>70.34</td>\n",
       "      <td>99.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Field  Exact Match Accuracy (%)  Average Fuzzy Match Score (%)\n",
       "0      findings                     76.52                          99.22\n",
       "1  clinicaldata                     83.23                          98.82\n",
       "2      ExamName                      0.63                          99.28\n",
       "3    impression                     70.34                          99.26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    if isinstance(s, str):\n",
    "        return s.strip().replace('\\n', ' ').replace('\\r', '').lower()\n",
    "    return \"\"\n",
    "\n",
    "# Clean both extracted and ground truth dataframes\n",
    "\n",
    "for field in fields:\n",
    "    extracted_df[field] = extracted_df[field].apply(clean_text)\n",
    "    ground_truth_df[field] = ground_truth_df[field].apply(clean_text)\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "results = []\n",
    "\n",
    "# Calculate exact match and fuzzy match for each field\n",
    "for field in fields:\n",
    "    exact_matches = (extracted_df[field] == ground_truth_df[field]).sum()\n",
    "    total = len(extracted_df)\n",
    "    exact_accuracy = exact_matches / total * 100\n",
    "\n",
    "    fuzzy_scores = [\n",
    "        fuzz.token_set_ratio(extracted_df[field][i], ground_truth_df[field][i])\n",
    "        for i in range(total)\n",
    "    ]\n",
    "    avg_fuzzy_score = sum(fuzzy_scores) / total\n",
    "\n",
    "    results.append({\n",
    "        \"Field\": field,\n",
    "        \"Exact Match Accuracy (%)\": round(exact_accuracy, 2),\n",
    "        \"Average Fuzzy Match Score (%)\": round(avg_fuzzy_score, 2)\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for display\n",
    "evaluation_df = pd.DataFrame(results)\n",
    "evaluation_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
